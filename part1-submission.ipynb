{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSA 2025 Phase 2 - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find all variables and understand them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "sales = pd.read_csv('sales.csv')\n",
    "features = pd.read_csv('features.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "\n",
    "# Display first 10 rows\n",
    "print(\"Sales Data (first 10 rows):\")\n",
    "print(sales.head(10))\n",
    "print(\"\\nFeatures Data (first 10 rows):\")\n",
    "print(features.head(10))\n",
    "print(\"\\nStores Data (first 10 rows):\")\n",
    "print(stores.head(10))\n",
    "\n",
    "# Statistical metrics\n",
    "print(\"\\nSales Data Statistics:\")\n",
    "print(sales.describe())\n",
    "print(\"\\nFeatures Data Statistics:\")\n",
    "print(features.describe())\n",
    "print(\"\\nStores Data Statistics:\")\n",
    "print(stores.describe())\n",
    "\n",
    "# Visualize Weekly_Sales (example)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(sales['Weekly_Sales'], bins=50, kde=True)\n",
    "plt.title('Weekly Sales Distribution')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=sales['Weekly_Sales'])\n",
    "plt.title('Weekly Sales Boxplot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data type conversion\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "sales['IsHoliday'] = sales['IsHoliday'].astype(int)\n",
    "features['Date'] = pd.to_datetime(features['Date'])\n",
    "features['IsHoliday'] = features['IsHoliday'].astype(int)\n",
    "for col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']:\n",
    "    features[col] = features[col].fillna(features[col].median())\n",
    "stores['Type'] = stores['Type'].map({'A': 0, 'B': 1, 'C': 2})\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = sales.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "merged_data = merged_data.merge(stores, on='Store', how='left')\n",
    "\n",
    "# Confirm data types\n",
    "print(\"\\nMerged Data Types:\")\n",
    "print(merged_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "sales = pd.read_csv('sales.csv')\n",
    "features = pd.read_csv('features.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "\n",
    "# Merge datasets\n",
    "sales['Date'] = pd.to_datetime(sales['Date'])\n",
    "features['Date'] = pd.to_datetime(features['Date'])\n",
    "merged_data = sales.merge(features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "merged_data = merged_data.merge(stores, on='Store', how='left')\n",
    "\n",
    "# --- Step 1: Handle missing values ---\n",
    "# Reason: Missing values can degrade model performance; imputation preserves data integrity.\n",
    "# MarkDown1-MarkDown5: Fill with 0 (assuming NA means no promotion)\n",
    "for col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
    "    merged_data[col] = merged_data[col].fillna(0)\n",
    "# CPI, Unemployment: Fill with median (robustness)\n",
    "merged_data['CPI'] = merged_data['CPI'].fillna(merged_data['CPI'].median())\n",
    "merged_data['Unemployment'] = merged_data['Unemployment'].fillna(merged_data['Unemployment'].median())\n",
    "# Check missing values\n",
    "print(\"Missing Values Statistics:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# --- Step 2: Handle outliers ---\n",
    "# Reason: Outliers can distort model training; use IQR method to trim extreme Weekly_Sales.\n",
    "Q1 = merged_data['Weekly_Sales'].quantile(0.25)\n",
    "Q3 = merged_data['Weekly_Sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "merged_data = merged_data[(merged_data['Weekly_Sales'] >= lower_bound) & \n",
    "                          (merged_data['Weekly_Sales'] <= upper_bound)]\n",
    "print(f\"Data size after removing outliers: {len(merged_data)}\")\n",
    "\n",
    "# --- Step 3: Convert non-numeric columns ---\n",
    "# Reason: Machine learning models require numeric input; one-hot encoding suits categorical variables.\n",
    "# Type: One-hot encoding\n",
    "merged_data = pd.get_dummies(merged_data, columns=['Type'], prefix='Type')\n",
    "# IsHoliday: Convert to 0/1\n",
    "merged_data['IsHoliday'] = merged_data['IsHoliday'].astype(int)\n",
    "# Extract date features\n",
    "merged_data['Year'] = merged_data['Date'].dt.year\n",
    "merged_data['Month'] = merged_data['Date'].dt.month\n",
    "merged_data['Week'] = merged_data['Date'].dt.isocalendar().week\n",
    "merged_data = merged_data.drop('Date', axis=1)  # Remove original date column\n",
    "\n",
    "# --- Step 4: Feature standardization ---\n",
    "# Reason: Features with different scales can bias the model; standardization ensures fairness.\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', \n",
    "            'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size', 'Year', 'Month', 'Week']\n",
    "merged_data[num_cols] = scaler.fit_transform(merged_data[num_cols])\n",
    "\n",
    "# --- Step 5: Handle data imbalance (for IsHoliday) ---\n",
    "# Reason: IsHoliday is imbalanced (fewer holiday weeks); oversampling improves model performance on holiday weeks.\n",
    "X = merged_data.drop(['Weekly_Sales'], axis=1)\n",
    "y = merged_data['Weekly_Sales']\n",
    "# Check IsHoliday distribution\n",
    "print(\"IsHoliday Distribution:\")\n",
    "print(merged_data['IsHoliday'].value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, merged_data['IsHoliday'])\n",
    "print(\"IsHoliday Distribution after Oversampling:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "# --- Step 6: Feature selection ---\n",
    "# Reason: Irrelevant features can add noise and reduce model performance; select important features for efficiency.\n",
    "selector = SelectKBest(score_func=f_regression, k=10)  # Select top 10 features\n",
    "selector.fit(X, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "print(\"Selected Features:\", selected_features)\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# --- Step 7: Save cleaned data ---\n",
    "cleaned_data = pd.concat([X_selected, y], axis=1)\n",
    "cleaned_data.to_csv('cleaned_data.csv', index=False)\n",
    "print(\"Cleaned data saved as 'cleaned_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- Visualization 1: Type (one-hot encoded as Type_A, Type_B, Type_C) ---\n",
    "# Purpose: Analyze the impact of different store types on Weekly_Sales\n",
    "plt.figure(figsize=(10, 6))\n",
    "type_cols = ['Type_0', 'Type_1', 'Type_2']  # Type_A, Type_B, Type_C\n",
    "type_sales = pd.melt(data, id_vars=['Weekly_Sales'], value_vars=type_cols, \n",
    "                     var_name='Type', value_name='IsType')\n",
    "type_sales = type_sales[type_sales['IsType'] == 1]\n",
    "type_sales['Type'] = type_sales['Type'].map({'Type_0': 'A', 'Type_1': 'B', 'Type_2': 'C'})\n",
    "sns.boxplot(x='Type', y='Weekly_Sales', data=type_sales)\n",
    "plt.title('Weekly Sales Distribution by Store Type')\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Weekly Sales (Standardized)')\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 2: IsHoliday ---\n",
    "# Purpose: Compare sales differences between holiday and non-holiday weeks\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='IsHoliday', y='Weekly_Sales', data=data)\n",
    "plt.title('Weekly Sales Distribution: Holiday vs Non-Holiday')\n",
    "plt.xlabel('Is Holiday (0=Non-Holiday, 1=Holiday)')\n",
    "plt.ylabel('Weekly Sales (Standardized)')\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 3: Time series aggregation (by month) ---\n",
    "# Purpose: Analyze monthly sales trends\n",
    "monthly_sales = data.groupby('Month')['Weekly_Sales'].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='Month', y='Weekly_Sales', data=monthly_sales, marker='o')\n",
    "plt.title('Average Weekly Sales Trend by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Weekly Sales (Standardized)')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 4: Time series aggregation (by year) ---\n",
    "# Purpose: Analyze yearly sales trends\n",
    "yearly_sales = data.groupby('Year')['Weekly_Sales'].mean().reset_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x='Year', y='Weekly_Sales', data=yearly_sales, marker='o')\n",
    "plt.title('Average Weekly Sales Trend by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Weekly Sales (Standardized)')\n",
    "plt.xticks(yearly_sales['Year'])\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 5: Weekly and holiday interaction analysis ---\n",
    "# Purpose: Explore patterns in weekly sales related to holidays\n",
    "weekly_holiday_sales = data.groupby(['Week', 'IsHoliday'])['Weekly_Sales'].mean().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Week', y='Weekly_Sales', hue='IsHoliday', data=weekly_holiday_sales)\n",
    "plt.title('Average Weekly Sales by Week (Holiday vs Non-Holiday)')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Average Weekly Sales (Standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = data.corr(method='pearson')\n",
    "\n",
    "# Visualize correlation matrix - heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Variables')\n",
    "plt.show()\n",
    "\n",
    "# Extract correlations with Weekly_Sales\n",
    "weekly_sales_corr = correlation_matrix['Weekly_Sales'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelations with Weekly_Sales:\")\n",
    "print(weekly_sales_corr)\n",
    "\n",
    "# Identify high correlation pairs (threshold: |corr| > 0.7)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))\n",
    "print(\"\\nHigh Correlation Pairs (|corr| > 0.7):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} and {pair[1]}: {pair[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the EDA, I kicked things off by loading the sales.csv, features.csv, and stores.csv datasets, peeking at the first 10 rows to get a feel for the data. I ran stats like means and standard deviations to spot trends, then threw in histograms, boxplots, and bar charts to visualize stuff like Weekly_Sales and Type. A correlation heatmap helped me see how variables play together, especially with Weekly_Sales.\n",
    "\n",
    "For preprocessing, I tackled missing values by filling MarkDown1-MarkDown5 with zeros (no promos, probably) and CPI and Unemployment with medians to keep things smooth. Outliers in Weekly_Sales got trimmed using IQR to avoid skewing the model. I turned Date into Year, Month, and Week, made IsHoliday a 0/1 flag, and one-hot encoded Type into Type_A, Type_B, Type_C. Numerical columns got standardized with StandardScaler for fairness, and I used SMOTE to balance the rare holiday weeks in IsHoliday. Finally, I picked the top 10 features with SelectKBest, keeping heavy hitters like Dept and Size while ditching redundant ones like Type encodings.\n",
    "\n",
    "What stood out? Sales spike hard in November/December—think Thanksgiving and Christmas—and tank in January/February. Bigger stores (Type A) rake in more cash, while Type C lags. Holidays boost sales a bit, especially during big weeks like Black Friday. MarkDown promos don’t always pack a punch unless paired with holidays. I also noticed Type and Size were super correlated, so I dropped Type to avoid overlap. The IsHoliday imbalance was a sneaky issue, fixed with SMOTE to give holidays a fair shot. All these steps cleaned the data up nice and tight, ready for modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1a2242e7081b4f0929018da2a8cc567af1f3cf95e7af08c98cfb4addbb6241a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
